{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron and Multi-Layer Perceptron\n",
    "\n",
    "First the imports, Always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks\n",
    "\n",
    "There are many ways of knitting the nodes of a neural network together, and each way results in a more or less complex behavior. Possibly the simplest of all topologies is the feed-forward network. Signals flow in one direction only; there is never any loop in the signal paths.\n",
    "\n",
    "![title](img/NNFL_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, ANN’s have a layered structure. The input layer picks up the input signals and passes them on to the next layer, the so-called ‘hidden’ layer. (Actually, there may be more than one hidden layer in a neural network.) Last comes the output layer that delivers the result.\n",
    "\n",
    "Note: Input layer is not a layer as such, and should not be called an input layer, rather, should be named only as \"Inputs\". However, since many of the textbooks do adhere to this notation, we use the term input layer here.\n",
    "\n",
    "![title](img/NNFL_2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like a biological neuron has dendrites to receive signals, a cell body to process them, and an axon to send signals out to other neurons, the artificial neuron has a number of input channels, a processing stage, and one output that can fan out to multiple other artificial neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Each input gets scaled up or down\n",
    "\n",
    "When a signal comes in, it gets multiplied by a weight value that is assigned to this particular input. That is, if a neuron has three inputs, then it has three weights that can be adjusted individually. During the learning phase, the neural network can adjust the weights based on the error of the last test result.\n",
    "\n",
    "#### 2) All signals are summed up\n",
    "\n",
    "In the next step, the modified input signals are summed up to a single value. In this step, an offset is also added to the sum. This offset is called bias. The neural network also adjusts the bias during the learning phase.\n",
    "\n",
    "This is where the magic happens! At the start, all the neurons have random weights and random biases. After each learning iteration, weights and biases are gradually shifted so that the next result is a bit closer to the desired output. This way, the neural network gradually moves towards a state where the desired patterns are “learned”.\n",
    "\n",
    "#### 3) Activation\n",
    "\n",
    "Finally, the result of the neuron’s calculation is turned into an output signal. This is done by feeding the result to an activation function (also called transfer function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic form of an activation function is a simple binary function that has only two possible results.\n",
    "\n",
    "![title](img/NNFL_3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns 1 if the input is positive or zero, and 0 for any negative input. A neuron whose activation function is a function like this is called a perceptron.\n",
    "\n",
    "![title](img/NNFL_4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single perceptron, as bare and simple as it might appear, is able to learn where this line is, and when it finished learning, it can tell whether a given point is above or below that line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Perceptron for a two input AND gate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "[-1. -1. -1.  1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Perceptron at 0x10612c630>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self,X,Y,eta,X_test):\n",
    "        self.w=np.random.uniform(1,2,X.shape[1]+1) #one weight for the bias\n",
    "        self.x=X\n",
    "        self.y=Y\n",
    "        self.e=eta\n",
    "        Perceptron.Train(self)\n",
    "        Perceptron.Test(self,X_test)\n",
    "    \n",
    "    def Train(self):\n",
    "        x_fin=np.array([np.ones(self.x.shape[0])])\n",
    "        self.x=np.concatenate((self.x,np.transpose(x_fin)),axis=1)\n",
    "        for i in range(100):\n",
    "            y_pred=np.dot(self.x,np.transpose(self.w))\n",
    "            y_pred[y_pred>=0]=1\n",
    "            y_pred[y_pred<0]=-1\n",
    "            err=self.y-y_pred\n",
    "            for j in range(3):\n",
    "                self.w=self.w + self.e*err[j]*self.x[j]\n",
    "\n",
    "    \n",
    "    def Test(self,X_test):\n",
    "        x_bias=np.array([np.ones(X_test.shape[0])])\n",
    "        X_test=np.concatenate((X_test,np.transpose(x_bias)),axis=1)\n",
    "        y_test_pred=np.dot(X_test,np.transpose(self.w))\n",
    "        y_test_pred[y_test_pred>=0]=1\n",
    "        y_test_pred[y_test_pred<0]=-1\n",
    "        print(y_test_pred)\n",
    "        \n",
    "\n",
    "X=np.array([[-1,-1],[-1,1],[1,-1],[1,1]])\n",
    "Y=np.array([-1,-1,-1,1])\n",
    "X_test=np.array([[-1,-1],[-1,1],[1,-1],[1,1]])\n",
    "Perceptron(X,Y,0.01,X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets try the same thing with a XOR Gate using MLP\n",
    "\n",
    "Let's initialize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input array\n",
    "X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1],[0,0,0,1]])\n",
    "\n",
    "#Output\n",
    "y=np.array([[0],[1],[0],[1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multi-Layered Perceptron\n",
    "\n",
    "As we can see the XOR Gate fails on a single perceptron. A look at the graph will give us enough intuition as to why this is the case.\n",
    "\n",
    "![title](img/xor.png)\n",
    "\n",
    " We need two lines instead of the one generated by the perceptron. Since we obviously can't use a single perceptron for this case we have to make our architecture slightly more complex. We'll use the backpropagation algorithm. This has two phases :\n",
    " \n",
    " 1) <b>Forward Propagation</b> (Exactly what happens during a normal perceptron)\n",
    " \n",
    " 2) <b>Backpropagation</b> (Calculating the errors and updating weights. This differs between layers)\n",
    "\n",
    "![title](img/backprop1.png)\n",
    "\n",
    "![title](img/backprop2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function (Activation)\n",
    "\n",
    "Lets make the activation function that we'll be using. This activation function is called sigmoid and is extremely useful because of one particular property that makes it valuable during backpropagation. It also squashes the output between 0 and 1 making it easier for us to calculate probabilities for the output signals.\n",
    "#### This is the equation\n",
    "![title](img/sigmoid_equation.png)\n",
    "\n",
    "\n",
    "![title](img/sigmoid_curve.png)\n",
    "\n",
    "\n",
    "We see that when we diffrentiate this equation we get a very simple equation :\n",
    "![title](img/sigmoid_derivative.jpg)\n",
    " \n",
    "\n",
    "Using this during the backpropagation phase of the algorithm is computationally less intensive than calculating other gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling the MLP together\n",
    "\n",
    "### First initialize all the variables and set the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable initialization\n",
    "epoch=5000 #Setting training iterations\n",
    "lr=0.1 #Setting learning rate\n",
    "inputlayer_neurons = X.shape[1] #number of features in data set\n",
    "hiddenlayer_neurons = 3 #number of hidden layers neurons\n",
    "output_neurons = 1 #number of neurons at output layer\n",
    "\n",
    "#weight and bias initialization\n",
    "np.random.seed(420)\n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
    "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bout=np.random.uniform(size=(1,output_neurons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now for the actual algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04268954]\n",
      " [ 0.96073561]\n",
      " [ 0.03243673]\n",
      " [ 0.99317504]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "\n",
    "    #Forward Propogation\n",
    "    hidden_layer_input1=np.dot(X,wh)\n",
    "    hidden_layer_input=hidden_layer_input1 + bh\n",
    "    hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "    output_layer_input1=np.dot(hiddenlayer_activations,wout)\n",
    "    output_layer_input= output_layer_input1+ bout\n",
    "    output = sigmoid(output_layer_input)\n",
    "\n",
    "    #Backpropagation\n",
    "\n",
    "    Error = y - output\n",
    "    \n",
    "    # Slopes are found using derivatives of its output\n",
    "    slope_output_layer = derivatives_sigmoid(output)\n",
    "    slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
    "    \n",
    "    # Found using multiplication of Error and ____\n",
    "    delta_output = Error * slope_output_layer\n",
    "    \n",
    "    \n",
    "    Error_at_hidden_layer = delta_output.dot(wout.T)\n",
    "    \n",
    "    # Similar to delta_output\n",
    "    delta_hidden_layer = Error_at_hidden_layer * slope_hidden_layer\n",
    "    \n",
    "    # Weight and Bias Update\n",
    "    wout += hiddenlayer_activations.T.dot(delta_output)*lr\n",
    "    bout += np.sum(delta_output, axis=0,keepdims=True) *lr\n",
    "    wh += X.T.dot(delta_hidden_layer)*lr\n",
    "    bh += np.sum(delta_hidden_layer, axis=0,keepdims=True) *lr\n",
    "\n",
    "print (output)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
